{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a1bb69-93b8-41cb-a5ee-d9a2afc839ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid BiLSTM (text) + MLP (tabular) for disaster tweet classification using your engineered features.\n",
    "# Expects preprocessed DataFrames `train_fe` and `test_fe` in memory (as created by your build_features()).\n",
    "# Will prefer the 'text_kw' column if present, else fallback to 'text_clean'.\n",
    "from __future__ import annotations\n",
    "\n",
    "import html\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84afcd0b-b438-4472-8f83-6c0730e97e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Reproducibility and device\n",
    "# -------------------------\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e678c81-7543-4274-967f-fd80344f212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Tokenization (simple whitespace; text is already normalized)\n",
    "# -------------------------\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return text.strip().split()\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, counter: Dict[str, int], min_freq: int = 2, max_size: Optional[int] = 30000):\n",
    "        # specials first\n",
    "        self.itos = [PAD_TOKEN, UNK_TOKEN]\n",
    "        self.stoi = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "        # sort by freq desc then lexicographically\n",
    "        items = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "        if max_size is not None:\n",
    "            items = items[:max_size]\n",
    "        for tok, freq in items:\n",
    "            if freq < min_freq:\n",
    "                continue\n",
    "            if tok not in self.stoi:\n",
    "                self.stoi[tok] = len(self.itos)\n",
    "                self.itos.append(tok)\n",
    "        self.pad_index = self.stoi[PAD_TOKEN]\n",
    "        self.unk_index = self.stoi[UNK_TOKEN]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        return [self.stoi.get(t, self.unk_index) for t in tokens]\n",
    "\n",
    "def build_vocab(texts: List[str], min_freq: int = 2, max_size: int = 30000) -> Vocab:\n",
    "    from collections import Counter\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(tokenize(t))\n",
    "    return Vocab(counter, min_freq=min_freq, max_size=max_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d50e906c-e8b6-4e45-a53e-0cb2504c713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Dataset / Collate\n",
    "# -------------------------\n",
    "class HybridDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        tabular: np.ndarray,\n",
    "        vocab: Vocab,\n",
    "        labels: Optional[np.ndarray] = None,\n",
    "        max_len: int = 60\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.tabular = tabular.astype(np.float32)\n",
    "        self.vocab = vocab\n",
    "        self.labels = labels.astype(np.float32) if labels is not None else None\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        toks = tokenize(self.texts[idx])\n",
    "        if self.max_len is not None and len(toks) > self.max_len:\n",
    "            toks = toks[: self.max_len]\n",
    "        ids = self.vocab.encode(toks)\n",
    "        item = {\n",
    "            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"length\": len(ids),\n",
    "            \"tab\": torch.tensor(self.tabular[idx], dtype=torch.float),\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item[\"label\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "def collate_fn(batch, pad_idx: int):\n",
    "    lengths = [b[\"length\"] for b in batch]\n",
    "    max_len = max(lengths) if lengths else 0\n",
    "    B = len(batch)\n",
    "    input_ids = torch.full((B, max_len), pad_idx, dtype=torch.long)\n",
    "    tabs = []\n",
    "    labels = []\n",
    "    for i, b in enumerate(batch):\n",
    "        ids = b[\"input_ids\"]\n",
    "        L = ids.size(0)\n",
    "        input_ids[i, :L] = ids\n",
    "        tabs.append(b[\"tab\"].unsqueeze(0))\n",
    "        if \"label\" in b:\n",
    "            labels.append(b[\"label\"].unsqueeze(0))\n",
    "    out = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"lengths\": torch.tensor(lengths, dtype=torch.long),\n",
    "        \"tab\": torch.cat(tabs, dim=0),\n",
    "    }\n",
    "    if labels:\n",
    "        out[\"labels\"] = torch.cat(labels, dim=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd09eabd-0970-40c6-b8fe-34a010b7bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Model: BiLSTM encoder + Tabular MLP + Fusion head\n",
    "# -------------------------\n",
    "class TextEncoderBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_size: int, pad_idx: int, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True, bidirectional=True, num_layers=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_dim = hidden_size * 2\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        emb = self.embedding(input_ids)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        # h_n: (num_layers*2, B, hidden)\n",
    "        h_f = h_n[-2]  # (B, hidden)\n",
    "        h_b = h_n[-1]  # (B, hidden)\n",
    "        h = torch.cat([h_f, h_b], dim=1)  # (B, 2*hidden)\n",
    "        return self.dropout(h)\n",
    "\n",
    "class TabularMLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int = 64, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.BatchNorm1d(in_dim),\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.out_dim = hidden\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class HybridClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        pad_idx: int,\n",
    "        embed_dim: int = 128,\n",
    "        hidden_size: int = 128,\n",
    "        txt_dropout: float = 0.3,\n",
    "        tab_in_dim: int = 0,\n",
    "        tab_hidden: int = 64,\n",
    "        tab_dropout: float = 0.3,\n",
    "        fusion_dropout: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.text_encoder = TextEncoderBiLSTM(vocab_size, embed_dim, hidden_size, pad_idx, dropout=txt_dropout)\n",
    "        self.use_tab = tab_in_dim > 0\n",
    "        if self.use_tab:\n",
    "            self.tab_encoder = TabularMLP(tab_in_dim, hidden=tab_hidden, dropout=tab_dropout)\n",
    "            fusion_in = self.text_encoder.out_dim + self.tab_encoder.out_dim\n",
    "        else:\n",
    "            self.tab_encoder = None\n",
    "            fusion_in = self.text_encoder.out_dim\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Dropout(fusion_dropout),\n",
    "            nn.Linear(fusion_in, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, lengths: torch.Tensor, tab: Optional[torch.Tensor] = None):\n",
    "        h_txt = self.text_encoder(input_ids, lengths)\n",
    "        if self.use_tab and tab is not None:\n",
    "            h_tab = self.tab_encoder(tab)\n",
    "            h = torch.cat([h_txt, h_tab], dim=1)\n",
    "        else:\n",
    "            h = h_txt\n",
    "        logit = self.fusion(h).squeeze(1)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc232ef2-5133-42c4-8b38-972c7423ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Training / Evaluation\n",
    "# -------------------------\n",
    "def batch_f1_from_logits(logits: torch.Tensor, labels: torch.Tensor, thr: float = 0.5) -> float:\n",
    "    probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "    preds = (probs >= thr).astype(np.int32)\n",
    "    y = labels.detach().cpu().numpy().astype(np.int32)\n",
    "    return f1_score(y, preds)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, clip: float = 1.0, thr: float = 0.5):\n",
    "    model.train()\n",
    "    losses, f1s = [], []\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        lengths = batch[\"lengths\"].to(device)\n",
    "        tab = batch[\"tab\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(input_ids, lengths, tab)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        f1s.append(batch_f1_from_logits(logits, labels, thr=thr))\n",
    "    return float(np.mean(losses)), float(np.mean(f1s))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, criterion, device, thr: float = 0.5):\n",
    "    model.eval()\n",
    "    losses, f1s = [], []\n",
    "    all_probs, all_true = [], []\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        lengths = batch[\"lengths\"].to(device)\n",
    "        tab = batch[\"tab\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, lengths, tab)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        f1s.append(f1_score(labels.cpu().numpy().astype(np.int32), (probs >= thr).astype(np.int32)))\n",
    "        all_probs.extend(probs.tolist())\n",
    "        all_true.extend(labels.cpu().numpy().astype(np.int32).tolist())\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(all_true, (np.array(all_probs) >= thr).astype(int), average=\"binary\", zero_division=0)\n",
    "    return float(np.mean(losses)), float(np.mean(f1s)), pr, rc, f1, np.array(all_probs), np.array(all_true)\n",
    "\n",
    "def find_best_threshold(probs: np.ndarray, y_true: np.ndarray, grid=None) -> float:\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.2, 0.8, 61)  # 0.2 to 0.8 step 0.01\n",
    "    best_thr, best_f1 = 0.5, -1.0\n",
    "    for t in grid:\n",
    "        f1 = f1_score(y_true, (probs >= t).astype(int))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, float(t)\n",
    "    return best_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "030fd5fd-7d66-4d87-b88d-09a36b1c150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Glue: prepare data and run\n",
    "# -------------------------\n",
    "DEFAULT_TABULAR_FEATURES = [\n",
    "    \"text_len\", \"word_count\",\n",
    "    \"url_count\", \"mention_count\", \"hashtag_count\",\n",
    "    \"has_url\", \"has_mention\", \"has_hashtag\",\n",
    "    \"has_location\", \"has_keyword\",\n",
    "    \"keyword_te\",\n",
    "]\n",
    "\n",
    "def select_text_column(df: pd.DataFrame, prefer: str = \"text_kw\", fallback: str = \"text_clean\") -> str:\n",
    "    if prefer in df.columns:\n",
    "        return prefer\n",
    "    if fallback in df.columns:\n",
    "        return fallback\n",
    "    raise KeyError(f\"Neither {prefer} nor {fallback} found in DataFrame.\")\n",
    "\n",
    "def build_tab_matrix(df: pd.DataFrame, feature_names: List[str], stats: Optional[Dict[str, Tuple[float,float]]] = None):\n",
    "    X = df[feature_names].copy()\n",
    "    # separate continuous vs binary\n",
    "    continuous = [\"text_len\", \"word_count\", \"url_count\", \"mention_count\", \"hashtag_count\", \"keyword_te\"]\n",
    "    binary = [\"has_url\", \"has_mention\", \"has_hashtag\", \"has_location\", \"has_keyword\"]\n",
    "    # ensure existence\n",
    "    for col in feature_names:\n",
    "        if col not in X.columns:\n",
    "            raise KeyError(f\"Required feature '{col}' is missing from input DataFrame.\")\n",
    "    # standardize continuous with provided stats or fit\n",
    "    fitted_stats = {} if stats is None else dict(stats)\n",
    "    for col in continuous:\n",
    "        if col not in X.columns:\n",
    "            continue\n",
    "        if stats is None:\n",
    "            mu = float(X[col].mean())\n",
    "            sd = float(X[col].std(ddof=0)) or 1.0\n",
    "            fitted_stats[col] = (mu, sd)\n",
    "        else:\n",
    "            mu, sd = stats[col]\n",
    "        X[col] = (X[col] - mu) / (sd if sd != 0 else 1.0)\n",
    "    # cast binary to float\n",
    "    for col in binary:\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].astype(float)\n",
    "    return X.values.astype(np.float32), fitted_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac96c1e7-5c59-44af-8ee9-3e367eea4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    target_col: str = \"target\",\n",
    "    min_freq: int = 2,              # controls vocab pruning (used in build_vocab)\n",
    "    max_vocab_size: int = 30000,    # caps vocab size (used in build_vocab)\n",
    "    max_len: int = 60,              # max sequence length (used in HybridDataset)\n",
    "    embedding_dim: int = 128,       # text embedding size (used in HybridClassifier)\n",
    "    hidden_size: int = 128,         # BiLSTM hidden size per direction (used in HybridClassifier)\n",
    "    txt_dropout: float = 0.3,       # dropout on text encoder output (used in HybridClassifier)\n",
    "    tab_hidden: int = 64,           # hidden width for tabular MLP (used in HybridClassifier)\n",
    "    tab_dropout: float = 0.3,       # dropout in tabular MLP (used in HybridClassifier)\n",
    "    fusion_dropout: float = 0.3,    # dropout before final linear head (used in HybridClassifier)\n",
    "    batch_size: int = 64,           # DataLoader batch size\n",
    "    lr: float = 1e-3,               # optimizer learning rate\n",
    "    epochs: int = 10,               # max training epochs\n",
    "    early_stopping_patience: int = 3,  # early stopping patience on val F1\n",
    "    use_pos_weight: bool = False,   # toggles BCEWithLogitsLoss(pos_weight=...)\n",
    "    initial_threshold: float = 0.5, # starting decision threshold for F1 computation\n",
    "    feature_names: Optional[List[str]] = None,  # which tabular features to use\n",
    "    val_size: float = 0.15,         # validation split size\n",
    "    seed: int = 42,                 # random seed (reproducibility and stratified split)\n",
    "):\n",
    "    set_seed(seed)  # uses: seed\n",
    "\n",
    "    # Choose text column (prefers 'text_kw' if present)\n",
    "    text_col = select_text_column(train_df)\n",
    "    print(f\"Using text column: {text_col}\")\n",
    "\n",
    "    # Define tabular features\n",
    "    if feature_names is None:  # uses: feature_names\n",
    "        feature_names = [c for c in DEFAULT_TABULAR_FEATURES if c in train_df.columns]\n",
    "\n",
    "    # Split train/val\n",
    "    tr_df, val_df = train_test_split(  # uses: val_size and seed\n",
    "        train_df, test_size=val_size, random_state=seed, stratify=train_df[target_col]\n",
    "    )\n",
    "    print(f\"Train size: {len(tr_df)} | Val size: {len(val_df)}\")\n",
    "\n",
    "    # Build vocab on training text only\n",
    "    vocab = build_vocab(                # uses: min_freq, max_vocab_size\n",
    "        tr_df[text_col].tolist(), min_freq=min_freq, max_size=max_vocab_size\n",
    "    )\n",
    "    print(f\"Vocab size: {len(vocab)} (min_freq={min_freq}, max_size={max_vocab_size})\")\n",
    "\n",
    "    # Prepare tabular matrices with standardization (fit on train, apply to val/test)\n",
    "    Xtr_tab, tab_stats = build_tab_matrix(tr_df, feature_names, stats=None)\n",
    "    Xval_tab, _ = build_tab_matrix(val_df, feature_names, stats=tab_stats)\n",
    "    Xte_tab, _ = build_tab_matrix(test_df, feature_names, stats=tab_stats)\n",
    "\n",
    "    # Prepare datasets/loaders\n",
    "    tr_ds = HybridDataset(              # uses: max_len\n",
    "        tr_df[text_col].tolist(), Xtr_tab, vocab, labels=tr_df[target_col].values, max_len=max_len\n",
    "    )\n",
    "    val_ds = HybridDataset(\n",
    "        val_df[text_col].tolist(), Xval_tab, vocab, labels=val_df[target_col].values, max_len=max_len\n",
    "    )\n",
    "    te_ds = HybridDataset(\n",
    "        test_df[text_col].tolist(), Xte_tab, vocab, labels=None, max_len=max_len\n",
    "    )\n",
    "\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True,\n",
    "                           collate_fn=lambda b: collate_fn(b, vocab.pad_index))  # uses: batch_size\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
    "                            collate_fn=lambda b: collate_fn(b, vocab.pad_index))\n",
    "    te_loader = DataLoader(te_ds, batch_size=batch_size, shuffle=False,\n",
    "                           collate_fn=lambda b: collate_fn(b, vocab.pad_index))\n",
    "\n",
    "    # Model\n",
    "    model = HybridClassifier(           # uses: embedding_dim, hidden_size, txt_dropout, tab_hidden, tab_dropout, fusion_dropout\n",
    "        vocab_size=len(vocab),\n",
    "        pad_idx=vocab.pad_index,\n",
    "        embed_dim=embedding_dim,\n",
    "        hidden_size=hidden_size,\n",
    "        txt_dropout=txt_dropout,\n",
    "        tab_in_dim=Xtr_tab.shape[1],\n",
    "        tab_hidden=tab_hidden,\n",
    "        tab_dropout=tab_dropout,\n",
    "        fusion_dropout=fusion_dropout,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Loss\n",
    "    if use_pos_weight:                  # uses: use_pos_weight\n",
    "        pos_ratio = tr_df[target_col].mean()\n",
    "        pos_weight = torch.tensor([(1.0 - pos_ratio) / max(pos_ratio, 1e-6)], dtype=torch.float, device=DEVICE)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        print(f\"Using pos_weight={pos_weight.item():.4f}\")\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "    # Training loop with early stopping on val F1\n",
    "    best_val_f1, best_state, patience = -1.0, None, 0\n",
    "    thr = initial_threshold                                   # uses: initial_threshold\n",
    "\n",
    "    for epoch in range(1, epochs + 1):                        # uses: epochs\n",
    "        tr_loss, tr_f1 = train_one_epoch(model, tr_loader, optimizer, criterion, DEVICE, thr=thr)\n",
    "        val_loss, _, pr, rc, f1, val_probs, val_true = eval_epoch(model, val_loader, criterion, DEVICE, thr=thr)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Threshold tuning each epoch to maximize F1 on current model\n",
    "        thr = find_best_threshold(val_probs, val_true)\n",
    "        _, _, pr, rc, f1, _, _ = eval_epoch(model, val_loader, criterion, DEVICE, thr=thr)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}/{epochs} | \"\n",
    "              f\"train_loss={tr_loss:.4f} train_f1~={tr_f1:.4f} | \"\n",
    "              f\"val_loss={val_loss:.4f} val_f1={f1:.4f} (P={pr:.3f}, R={rc:.3f}) | thr={thr:.3f}\")\n",
    "\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_stopping_patience:           # uses: early_stopping_patience\n",
    "                print(f\"Early stopping at epoch {epoch}. Best val F1: {best_val_f1:.4f}\")\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})\n",
    "\n",
    "    # Final val metrics at tuned threshold\n",
    "    val_loss, _, pr, rc, f1, val_probs, val_true = eval_epoch(model, val_loader, criterion, DEVICE, thr=thr)\n",
    "    thr = find_best_threshold(val_probs, val_true)  # final tune\n",
    "    _, _, pr, rc, f1, _, _ = eval_epoch(model, val_loader, criterion, DEVICE, thr=thr)\n",
    "\n",
    "    print(\"\\nFinal Validation Metrics:\")\n",
    "    print(f\"Threshold={thr:.3f} Precision={pr:.4f} Recall={rc:.4f} F1={f1:.4f}\")\n",
    "\n",
    "    # Detailed report\n",
    "    y_pred = (val_probs >= thr).astype(int)\n",
    "    print(\"\\nClassification report (val):\")\n",
    "    print(classification_report(val_true, y_pred, digits=4))\n",
    "\n",
    "    # Inference on test\n",
    "    model.eval()\n",
    "    test_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in te_loader:\n",
    "            logits = model(batch[\"input_ids\"].to(DEVICE), batch[\"lengths\"].to(DEVICE), batch[\"tab\"].to(DEVICE))\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().tolist()\n",
    "            test_probs.extend(probs)\n",
    "    test_preds = [1 if p >= thr else 0 for p in test_probs]\n",
    "\n",
    "    # Save submission\n",
    "    if \"id\" in test_df.columns:\n",
    "        submission = pd.DataFrame({\"id\": test_df[\"id\"].values, \"target\": test_preds})\n",
    "    else:\n",
    "        submission = pd.DataFrame({\"target\": test_preds})\n",
    "    out_path = \"submission_hybrid_lstm.csv\"\n",
    "    submission.to_csv(out_path, index=False)\n",
    "    print(f\"\\nSaved submission to: {out_path}\")\n",
    "\n",
    "    artifacts = {\n",
    "        \"vocab\": vocab,\n",
    "        \"feature_names\": feature_names,\n",
    "        \"tab_stats\": tab_stats,\n",
    "        \"threshold\": thr,\n",
    "        \"best_val_f1\": best_val_f1,\n",
    "    }\n",
    "    return model, artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb4f01ef-94f0-45fe-94d8-0b450ed13808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompiled regex patterns\n",
    "_URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", flags=re.IGNORECASE)\n",
    "_MENTION_RE = re.compile(r\"@\\w+\")\n",
    "_HASHTAG_RE = re.compile(r\"#\\w+\")\n",
    "_HASHTAG_TOKEN_RE = re.compile(r\"#(\\w+)\")  # capture token without '#'\n",
    "\n",
    "def _to_str(x: object) -> str:\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return str(x)\n",
    "\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize tweet text:\n",
    "      - HTML unescape\n",
    "      - URLs -> 'URL'\n",
    "      - @mentions -> 'USER'\n",
    "      - hashtags: remove '#' but keep the word\n",
    "      - lowercase\n",
    "      - normalize whitespace\n",
    "    \"\"\"\n",
    "    s = _to_str(s)\n",
    "    s = html.unescape(s)\n",
    "\n",
    "    # Replace URLs and mentions\n",
    "    s = _URL_RE.sub(\"URL\", s)\n",
    "    s = _MENTION_RE.sub(\"USER\", s)\n",
    "\n",
    "    # Keep hashtag token (remove '#')\n",
    "    s = _HASHTAG_TOKEN_RE.sub(r\"\\1\", s)\n",
    "\n",
    "    # Lowercase and collapse whitespace\n",
    "    s = s.lower()\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def normalize_keyword(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize keyword string:\n",
    "      - Replace %20 and '-' with space\n",
    "      - lowercase\n",
    "      - normalize whitespace\n",
    "    \"\"\"\n",
    "    s = _to_str(s)\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.replace(\"%20\", \" \")\n",
    "    s = s.replace(\"-\", \" \")\n",
    "    s = s.lower()\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def normalize_location(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Light location normalization:\n",
    "      - lowercase\n",
    "      - normalize whitespace\n",
    "    \"\"\"\n",
    "    s = _to_str(s)\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def extract_text_counters(raw_text: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count social/media tokens from the RAW text (pre-cleaning):\n",
    "      - url_count\n",
    "      - mention_count\n",
    "      - hashtag_count\n",
    "    \"\"\"\n",
    "    s = _to_str(raw_text)\n",
    "    url_count = len(_URL_RE.findall(s))\n",
    "    mention_count = len(_MENTION_RE.findall(s))\n",
    "    hashtag_count = len(_HASHTAG_RE.findall(s))\n",
    "    return {\n",
    "        \"url_count\": url_count,\n",
    "        \"mention_count\": mention_count,\n",
    "        \"hashtag_count\": hashtag_count,\n",
    "    }\n",
    "\n",
    "def _cv_target_encode(\n",
    "    cat_series: pd.Series,\n",
    "    y: pd.Series,\n",
    "    n_splits: int = 5,\n",
    "    random_state: int = 42,\n",
    "    smoothing_min_samples: int = 1,\n",
    ") -> Tuple[pd.Series, Dict[str, float], float]:\n",
    "    \"\"\"\n",
    "    Cross-validated target encoding for a single categorical column.\n",
    "    - Returns:\n",
    "        oof_te: pd.Series of out-of-fold encoded values for training data.\n",
    "        full_mapping: dict mapping category -> mean(target) on full training.\n",
    "        global_mean: float, overall positive rate used as fallback.\n",
    "    - Note: simple unsmoothed mean; can be extended with smoothing if needed.\n",
    "    \"\"\"\n",
    "    cat_series = cat_series.astype(str).fillna(\"\")\n",
    "    y = y.astype(int).values\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    oof = np.zeros(len(cat_series), dtype=float)\n",
    "\n",
    "    # Precompute global mean\n",
    "    global_mean = float(np.mean(y))\n",
    "\n",
    "    # Build OOF means\n",
    "    for train_idx, valid_idx in skf.split(np.zeros(len(y)), y):\n",
    "        # Means in the training fold\n",
    "        fold_map = (\n",
    "            pd.DataFrame({\"cat\": cat_series.iloc[train_idx].values, \"y\": y[train_idx]})\n",
    "            .groupby(\"cat\")[\"y\"]\n",
    "            .mean()\n",
    "            .to_dict()\n",
    "        )\n",
    "        # Assign to validation fold\n",
    "        oof_fold = [\n",
    "            fold_map.get(cat_series.iloc[i], global_mean) for i in valid_idx\n",
    "        ]\n",
    "        oof[valid_idx] = np.array(oof_fold, dtype=float)\n",
    "\n",
    "    # Full mapping for inference on test\n",
    "    full_mapping = (\n",
    "        pd.DataFrame({\"cat\": cat_series.values, \"y\": y})\n",
    "        .groupby(\"cat\")[\"y\"]\n",
    "        .mean()\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    return pd.Series(oof, index=cat_series.index), full_mapping, global_mean\n",
    "\n",
    "def build_features(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    *,\n",
    "    text_col: str = \"text\",\n",
    "    keyword_col: str = \"keyword\",\n",
    "    location_col: str = \"location\",\n",
    "    target_col: str = \"target\",\n",
    "    add_keyword_to_text: bool = True,\n",
    "    n_splits: int = 5,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, float], float]:\n",
    "    \"\"\"\n",
    "    Build cleaned columns and numeric features for the Kaggle disaster tweets dataset.\n",
    "\n",
    "    Returns:\n",
    "      train_out, test_out, keyword_te_mapping, keyword_te_global_mean\n",
    "\n",
    "    Output columns:\n",
    "      - text_clean, keyword_clean, location_clean\n",
    "      - text_kw (if add_keyword_to_text=True)\n",
    "      - text_len, word_count\n",
    "      - url_count, mention_count, hashtag_count\n",
    "      - has_url, has_mention, has_hashtag, has_location, has_keyword\n",
    "      - keyword_te\n",
    "      - id (if present), target (train only)\n",
    "    \"\"\"\n",
    "    # Copy to avoid mutating user inputs\n",
    "    train = train_df.copy()\n",
    "    test = test_df.copy()\n",
    "\n",
    "    # 1) Normalize keyword and location\n",
    "    train[\"keyword_clean\"] = train[keyword_col].map(normalize_keyword)\n",
    "    test[\"keyword_clean\"] = test[keyword_col].map(normalize_keyword)\n",
    "\n",
    "    train[\"location_clean\"] = train[location_col].map(normalize_location)\n",
    "    test[\"location_clean\"] = test[location_col].map(normalize_location)\n",
    "\n",
    "    # 2) Extract counters from RAW text first (before cleaning replacements)\n",
    "    train_counts = train[text_col].map(extract_text_counters).apply(pd.Series)\n",
    "    test_counts = test[text_col].map(extract_text_counters).apply(pd.Series)\n",
    "\n",
    "    # 3) Clean text\n",
    "    train[\"text_clean\"] = train[text_col].map(normalize_text)\n",
    "    test[\"text_clean\"] = test[text_col].map(normalize_text)\n",
    "\n",
    "    # 4) Optionally prepend keyword tag to text\n",
    "    if add_keyword_to_text:\n",
    "        def _prepend_kw(row):\n",
    "            kw = row[\"keyword_clean\"]\n",
    "            t = row[\"text_clean\"]\n",
    "            return f\"[kw={kw}] {t}\" if kw else t\n",
    "\n",
    "        train[\"text_kw\"] = train[[\"keyword_clean\", \"text_clean\"]].apply(_prepend_kw, axis=1)\n",
    "        test[\"text_kw\"] = test[[\"keyword_clean\", \"text_clean\"]].apply(_prepend_kw, axis=1)\n",
    "\n",
    "    # 5) Length and token counts from CLEANED text\n",
    "    train[\"text_len\"] = train[\"text_clean\"].str.len()\n",
    "    test[\"text_len\"] = test[\"text_clean\"].str.len()\n",
    "\n",
    "    train[\"word_count\"] = train[\"text_clean\"].str.split().str.len()\n",
    "    test[\"word_count\"] = test[\"text_clean\"].str.split().str.len()\n",
    "\n",
    "    # 6) Social token counts and binary flags\n",
    "    for col in [\"url_count\", \"mention_count\", \"hashtag_count\"]:\n",
    "        train[col] = train_counts[col].astype(int)\n",
    "        test[col] = test_counts[col].astype(int)\n",
    "\n",
    "    train[\"has_url\"] = (train[\"url_count\"] > 0).astype(int)\n",
    "    test[\"has_url\"] = (test[\"url_count\"] > 0).astype(int)\n",
    "\n",
    "    train[\"has_mention\"] = (train[\"mention_count\"] > 0).astype(int)\n",
    "    test[\"has_mention\"] = (test[\"mention_count\"] > 0).astype(int)\n",
    "\n",
    "    train[\"has_hashtag\"] = (train[\"hashtag_count\"] > 0).astype(int)\n",
    "    test[\"has_hashtag\"] = (test[\"hashtag_count\"] > 0).astype(int)\n",
    "\n",
    "    # 7) Presence flags for keyword/location\n",
    "    train[\"has_keyword\"] = (train[\"keyword_clean\"] != \"\").astype(int)\n",
    "    test[\"has_keyword\"] = (test[\"keyword_clean\"] != \"\").astype(int)\n",
    "\n",
    "    train[\"has_location\"] = (train[\"location_clean\"] != \"\").astype(int)\n",
    "    test[\"has_location\"] = (test[\"location_clean\"] != \"\").astype(int)\n",
    "\n",
    "    # 8) Leakage-safe target encoding for keyword\n",
    "    if target_col in train.columns:\n",
    "        oof_te, mapping, global_mean = _cv_target_encode(\n",
    "            train[\"keyword_clean\"], train[target_col],\n",
    "            n_splits=n_splits, random_state=random_state\n",
    "        )\n",
    "        train[\"keyword_te\"] = oof_te.values\n",
    "        test[\"keyword_te\"] = test[\"keyword_clean\"].map(mapping).fillna(global_mean).astype(float)\n",
    "    else:\n",
    "        # In case target is missing (rare), just pass through means as NaN\n",
    "        mapping, global_mean = {}, np.nan\n",
    "        train[\"keyword_te\"] = np.nan\n",
    "        test[\"keyword_te\"] = np.nan\n",
    "\n",
    "    # 9) Keep id/target if present\n",
    "    keep_cols_train = []\n",
    "    keep_cols_test = []\n",
    "\n",
    "    for col in [\"id\"]:\n",
    "        if col in train.columns:\n",
    "            keep_cols_train.append(col)\n",
    "        if col in test.columns:\n",
    "            keep_cols_test.append(col)\n",
    "\n",
    "    # Order columns for readability\n",
    "    base_cols = [\"text_clean\", \"keyword_clean\", \"location_clean\"]\n",
    "    if add_keyword_to_text:\n",
    "        base_cols.append(\"text_kw\")\n",
    "\n",
    "    feat_cols = [\n",
    "        \"text_len\", \"word_count\",\n",
    "        \"url_count\", \"mention_count\", \"hashtag_count\",\n",
    "        \"has_url\", \"has_mention\", \"has_hashtag\",\n",
    "        \"has_location\", \"has_keyword\",\n",
    "        \"keyword_te\",\n",
    "    ]\n",
    "\n",
    "    ordered_train_cols = keep_cols_train + base_cols + feat_cols\n",
    "    ordered_test_cols = keep_cols_test + base_cols + feat_cols\n",
    "\n",
    "    if target_col in train.columns:\n",
    "        ordered_train_cols = keep_cols_train + [target_col] + base_cols + feat_cols\n",
    "\n",
    "    train_out = train[ordered_train_cols].copy()\n",
    "    test_out = test[ordered_test_cols].copy()\n",
    "\n",
    "    return train_out, test_out, mapping, global_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "405f123c-9225-49da-a42a-38055b46469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_DIR = Path(\"data\")\n",
    "assert (DATA_DIR / \"train.csv\").exists(), \"Expected data/train.csv to exist\"\n",
    "assert (DATA_DIR / \"test.csv\").exists(), \"Expected data/test.csv to exist\"\n",
    "assert (DATA_DIR / \"sample_submission.csv\").exists(), \"Expected data/sample_submission.csv to exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0fea37b-4951-4a92-b728-50fec7722de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, logging\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"      # env-level suppression\n",
    "warnings.filterwarnings(\"ignore\")            # blanket ignore\n",
    "logging.getLogger().setLevel(logging.ERROR)  # silence most logged warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04c4ffad-e977-4c1d-b7db-1b7ffbcb892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "test = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "\n",
    "train_fe, test_fe, kw_mapping, kw_global = build_features(\n",
    "    train, test,\n",
    "    add_keyword_to_text=True,\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2df75478-53f7-4636-a9ce-705157e06008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using text column: text_kw\n",
      "Train size: 6471 | Val size: 1142\n",
      "Vocab size: 4346 (min_freq=3, max_size=30000)\n",
      "Epoch 01/10 | train_loss=0.6110 train_f1~=0.5448 | val_loss=0.5266 val_f1=0.7168 (P=0.736, R=0.699) | thr=0.500\n",
      "Epoch 02/10 | train_loss=0.5096 train_f1~=0.7025 | val_loss=0.4948 val_f1=0.7378 (P=0.694, R=0.788) | thr=0.290\n",
      "Epoch 03/10 | train_loss=0.4670 train_f1~=0.7254 | val_loss=0.4703 val_f1=0.7445 (P=0.730, R=0.760) | thr=0.400\n",
      "Epoch 04/10 | train_loss=0.4343 train_f1~=0.7651 | val_loss=0.4682 val_f1=0.7522 (P=0.799, R=0.711) | thr=0.480\n",
      "Epoch 05/10 | train_loss=0.4026 train_f1~=0.7788 | val_loss=0.4852 val_f1=0.7513 (P=0.768, R=0.735) | thr=0.430\n",
      "Epoch 06/10 | train_loss=0.3610 train_f1~=0.8083 | val_loss=0.4848 val_f1=0.7393 (P=0.739, R=0.739) | thr=0.340\n",
      "Early stopping at epoch 6. Best val F1: 0.7522\n",
      "\n",
      "Final Validation Metrics:\n",
      "Threshold=0.480 Precision=0.7986 Recall=0.7108 F1=0.7522\n",
      "\n",
      "Classification report (val):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7986    0.8648    0.8304       651\n",
      "           1     0.7986    0.7108    0.7522       491\n",
      "\n",
      "    accuracy                         0.7986      1142\n",
      "   macro avg     0.7986    0.7878    0.7913      1142\n",
      "weighted avg     0.7986    0.7986    0.7967      1142\n",
      "\n",
      "\n",
      "Saved submission to: submission_hybrid_lstm.csv\n"
     ]
    }
   ],
   "source": [
    "model, artifacts = run_training(\n",
    "    train_df=train_fe,\n",
    "    test_df=test_fe,\n",
    "    target_col=\"target\",\n",
    "    # text capacity/noise\n",
    "    min_freq=3,            # was 2\n",
    "    max_vocab_size=30000,\n",
    "    max_len=50,            # was 60\n",
    "    embedding_dim=128,\n",
    "    hidden_size=112,       # was 128\n",
    "    # regularization\n",
    "    txt_dropout=0.45,      # was 0.3\n",
    "    tab_hidden=64,\n",
    "    tab_dropout=0.4,       # was 0.3\n",
    "    fusion_dropout=0.45,   # was 0.3\n",
    "    # optimization\n",
    "    batch_size=64,\n",
    "    lr=8e-4,               # was 1e-3 (weâ€™ll also add weight_decay below)\n",
    "    epochs=10,\n",
    "    early_stopping_patience=2,  # was 3; your peak comes early\n",
    "    use_pos_weight=False,\n",
    "    initial_threshold=0.5,\n",
    "    feature_names=None,\n",
    "    val_size=0.15,\n",
    "    seed=42,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
